%template by Marcel Neunhoeffer & Sebastian Sternberg - Uni of Mannheim

\documentclass[a4paper, 12pt]{article}  %khai bao document class

% set up margin
\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
% \usepackage[utf8]{inputenc}  % language encoder
\usepackage[utf8]{vietnam}  % vietnamese language setting
\usepackage{multirow} % Multirow is for tables with multiple rows within one cell.
\usepackage{booktabs} % For even nicer tables.
\usepackage{graphicx}
\usepackage{ulem} % for underlined format
% packages for advanced math eqn and symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem} % for itemize
\usepackage[colorlinks=true]{hyperref}
\usepackage[skip=8pt plus1pt]{parskip} % paragraph spacing

% set indent of new paragraph to 0
\usepackage{setspace}
\setlength{\parindent}{0 in}
\onehalfspacing

% Package to place figures where you want them.
\usepackage{float}

% The fancyhdr package let's us create nice headers.
\usepackage{fancyhdr}

%%Make header and footer
\pagestyle{fancy} % With this command we can customize the header style.
\fancyhf{} % This makes sure we do not have other information in our header or footer.

% Header
\lhead{\footnotesize Machine Learning 2: PCA}% \lhead puts text in the top left corner. \footnotesize sets our font to a smaller size.
\rhead{\footnotesize Nguyen Anh Tu @DSEB 62}

% Similar commands work for the footer (\lfoot, \cfoot and \rfoot).
% We want to put our page number in the center.
\cfoot{\footnotesize \thepage} 

%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{document}

% Title section of the document
\thispagestyle{empty} % This command disables the header on the first page. 

\begin{tabular}{p{12.5cm}} % This is a simple tabular environment to align your text nicely 
{\large \bf National Economics University, Vietnam} \\
Faculty of Mathematics Economics \\ Data Science in Economics and Business  \\ Machine Learning 2\\
\hline % \hline produces horizontal lines.
\\
\end{tabular} % Our tabular environment ends here.

\vspace*{0.3cm} % Now we want to add some vertical space in between the line and our title.

\begin{center} % Everything within the center environment is centered.
	{\Large \bf Homework Week 1: Principal Component Analysis}
	\vspace{2mm}
	
	{\bf Student: Nguyễn Anh Tú - ID: 11207333} % <---- Fill in your names here!
\end{center}  

%%%%%%%%%%%%%%%% Problem 1:
\section*{Problem 1}

Explain the math behind Principal Component Analysis algorithm.

%%%% Solution
\textbf{Solution.}

We consider an independent and identically distributed (i.i.d.) dataset $X
= {x_1, x_2, \hdots, x_n}$ with mean 0 and $x_n \in \mathbb{R}^D$. In general, if a dataset does not have a non-zero mean, we can standardize it before other steps.

PCA is the method in which we will find a new basis, a.k.a a projection matrix, where the original dataset can be projected onto it to reduce the data dimension but important information still retained.

We assume there exists a low-dimensional compressed representation of a data point $x_n$ given by:
\[z_n = B^T x_n\]
where we compressed $x_n$ of $D$ dimensions to $z_n$ of $M$ dimensions $(M < D)$, and the projection matrix is:
\[B = [b_1, b_2, \hdots, b_M] \in \mathbb{R}^{D \times M}\]
Note that $z$ also has zero mean: \(\mathbf{E}_z[z] = \mathbf{E}_x[B^T x] = B^T \mathbf{E}_x[x] = 0\)

We need to find a matrix $B$ that retains as much information as possible when compressing data by projecting it onto the subspace spanned by the columns $b_1, b_2, \hdots, b_M$ of $B$. Retaining most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional code.

We maximize the variance of the low-dimensional code using a sequential approach. First, we consider the case when $X$ is projected onto a single vector $b \in \mathbb{R}^D$ (basically, this is when the projection matrix $B$ is just a vector and we want to keep only one most important feature). Our aim is to maximize the variance of the projected data, i.e: \[\text{maximize } V = \frac{1}{N} \sum^N_{n = 1} z_n^2\]
where \(z_n = b^T x_n\). Substituting this into the expression of $V$:
\begin{align*}
    V &= \frac{1}{N} \sum^N_{n = 1} (b^T x_n)^2 = \frac{1}{N} \sum^N_{n = 1} b^T x_n x_n^T b\\
    &= b^T (\frac{1}{N} \sum^N_{n = 1} x_n x_n^T) b = b^T S b
\end{align*}
where $S$ is the covariance matrix of the original dataset. 

Here, we observe that increasing the magnitude of $b$ will increase $V$, thus making this optimization problem impossible to solve. Therefore, we restrict all solutions to $||b||^2 = 1$ which results in a constrained optimization problem:
\begin{align*}
    &\text{find } \underset{b}{\mathrm{argmax}} \,\, b^T S b \\
    &\text{subject to } ||b||^2 = 1
\end{align*}
The Lagrangian function for the problem is:
\[\mathcal{L}(b, \lambda) = b^T S b + \lambda(1 - b^T b)\]
Note that $b$ is a single vector then $\lambda$ is just a number (not a vector) because we only have one condition. Take the partial derivative of $\mathcal{L}$ with respect to $b$ and $\lambda$ to 0:
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial b} &= 2 b^T S + 2 \lambda b^T = 0 \Longleftrightarrow S\cdot b = \lambda b \\
    \frac{\partial \mathcal{L}}{\partial \lambda} &= 1 - b^T b \Longleftrightarrow b^T b = 1
\end{align*}
At this point, we can see that $b$ and $\lambda$ are a pair of eigenvector and eigenvalue of $S$. Then we can rewrite the variance $V$ as:
\[V = b^T S b = b^T \lambda b = \lambda\]
The variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector $b$ that spans this subspace.

Therefore, to maximize the variance of the low-dimensional code, we choose the basis vector associated with the largest eigenvalue principal component of the data covariance matrix. This eigenvector is called the first principal component.

The finding of vector $b$ above can be viewed as the finding of a direction or an axis that data varies the most. Suppose we want to find another direction besides the one we have found, we can construct the similar vector projection problem to find the largest variance possible, or actually the largest eigenvalue possible. Now what we derive will be the second largest eigenvalue. Therefore, if we want to find $M$ axes to project data, we can choose $M$ eigenvectors $b_1, b_2, \hdots, b_M$ of the data variance matrix that associates with $M$ largest eigenvalues, each eigenvector corresponding to a new axis of projection. The projection matrix $B$ is formed by those eigenvectors, i.e $B = [b_1, b_2, \hdots, b_M]$.

Finally, the projection of dataset $X \in \mathbb{R}^{D \times N}$ onto $B \in \mathbb{R}^{D \times M}$ given by $\widetilde{X} = B^T X$ will result in new representation of data, i. e $\widetilde{X} \in \mathbb{R}^{M \times N}$ (M features and N samples). 

\textbf{Note} In case me in the future forget how this algorithm works, remember features of $\widetilde{X}$ are not derived by removing some less important features of $X$ (and keeping the remaining) but by synthesizing information from the whole dataset $X$ and compressing them into $M$ \textbf{new} features.

Summary (\href{https://machinelearningcoban.com/2017/06/15/pca/#4-cac-bc-thc-hin-pca}{image source}):
\begin{center}
  \includegraphics[width=0.8\textwidth]{pca summary.jpg}  
\end{center}

\end{document}
